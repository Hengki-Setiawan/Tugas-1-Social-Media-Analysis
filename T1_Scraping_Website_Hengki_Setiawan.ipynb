{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRYx/UCnllQSiZMzttNjlJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hengki-Setiawan/Tugas-1-Social-Media-Analysis/blob/main/T1_Scraping_Website_Hengki_Setiawan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping portal berita"
      ],
      "metadata": {
        "id": "65Is3Rs92fmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**instal di terminal**\n",
        "pip install beautifulsoup4 requests pandas / pip install beautifulsoup4 requests pandas openpyxl / pip install openpyxl"
      ],
      "metadata": {
        "id": "Ov_nXTab2oH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lanjut kan ke Proses Scraping"
      ],
      "metadata": {
        "id": "qIvsoxkj2ycq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4NOpyVK2TJ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6362df0d-2b6b-49cd-a714-9fdb7459ebbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mengakses URL: https://www.detik.com/\n",
            "Menemukan 104 total artikel.\n",
            "Berhasil mengumpulkan 68 artikel yang valid. Mengekspor ke file...\n",
            "Data berhasil diekspor ke list_post_detik_lengkap.json\n",
            "Data berhasil diekspor ke list_post_detik_lengkap.csv\n",
            "Data berhasil diekspor ke list_post_detik_lengkap.xlsx\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# URL dari halaman utama Detik.com\n",
        "url = 'https://www.detik.com/'\n",
        "\n",
        "print(\"Mengakses URL:\", url)\n",
        "# Mengirim permintaan HTTP ke URL target\n",
        "try:\n",
        "    # Menambahkan header User-Agent untuk menyamar sebagai browser biasa\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error saat mengakses URL: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Parsing HTML dari halaman web menggunakan BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Menyiapkan list untuk menampung semua data\n",
        "collected_data = []\n",
        "\n",
        "# Menemukan semua artikel. Di detik.com, artikel dibungkus dalam tag <article>\n",
        "articles = soup.find_all('article')\n",
        "print(f\"Menemukan {len(articles)} total artikel.\")\n",
        "\n",
        "for article in articles:\n",
        "    # Mencari judul (h2 atau h3)\n",
        "    title_tag = article.find('h2', class_='media__title') or article.find('h3', 'media__title')\n",
        "    if not title_tag:\n",
        "        continue\n",
        "\n",
        "    title = title_tag.get_text(strip=True)\n",
        "\n",
        "    # Mencari link dari tag <a> di dalam tag judul\n",
        "    link_tag = title_tag.find('a')\n",
        "    link = link_tag['href'] if link_tag and link_tag.has_attr('href') else 'Tidak ada link'\n",
        "\n",
        "    # --- PENAMBAHAN METADATA BARU ---\n",
        "\n",
        "    # 1. Mengambil URL Gambar Utama (Image URL)\n",
        "    # Gambar biasanya ada di dalam tag <img> di dalam elemen <figure> atau langsung <div>\n",
        "    image_tag = article.find('img')\n",
        "    image_url = image_tag['src'] if image_tag and image_tag.has_attr('src') else 'Tidak ada gambar'\n",
        "\n",
        "    # 2. Mengambil Nama Penulis (Author)\n",
        "    # Penulis seringkali ada di dalam span dengan class 'media__author'\n",
        "    author_tag = article.find('span', class_='media__author')\n",
        "    author = author_tag.get_text(strip=True) if author_tag else 'Tidak diketahui'\n",
        "\n",
        "    # --- METADATA LAMA ---\n",
        "\n",
        "    # Mencari waktu publikasi\n",
        "    time_tag = article.find('span', class_='media__date-content')\n",
        "    publication_time = time_tag.get_text(strip=True) if time_tag else 'Tidak ada waktu'\n",
        "\n",
        "    # Konten/summary (opsional)\n",
        "    content_tag = article.find('p', class_='media__summary')\n",
        "    summary = content_tag.get_text(strip=True) if content_tag else 'Tidak ada ringkasan'\n",
        "\n",
        "    # Mengumpulkan data jika judul dan link ditemukan\n",
        "    if title and link != 'Tidak ada link':\n",
        "        article_data = {\n",
        "            'title': title,\n",
        "            'link': link,\n",
        "            'publication_time': publication_time,\n",
        "            'author': author,\n",
        "            'image_url': image_url,\n",
        "            'summary': summary\n",
        "        }\n",
        "        collected_data.append(article_data)\n",
        "\n",
        "# --- Proses Ekspor Hasil ---\n",
        "if not collected_data:\n",
        "    print(\"Tidak ada data yang berhasil di-scrape. Mungkin struktur website telah berubah.\")\n",
        "else:\n",
        "    print(f\"Berhasil mengumpulkan {len(collected_data)} artikel yang valid. Mengekspor ke file...\")\n",
        "\n",
        "    # Mengubah nama file output\n",
        "    file_name_base = 'list_post_detik_lengkap'\n",
        "\n",
        "    output_json_data = {\"list_post\": collected_data}\n",
        "    with open(f'{file_name_base}.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_json_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Data berhasil diekspor ke {file_name_base}.json\")\n",
        "\n",
        "    df = pd.DataFrame(collected_data)\n",
        "    df.to_csv(f'{file_name_base}.csv', index=False, encoding='utf-8')\n",
        "    print(f\"Data berhasil diekspor ke {file_name_base}.csv\")\n",
        "\n",
        "    df.to_excel(f'{file_name_base}.xlsx', index=False)\n",
        "    print(f\"Data berhasil diekspor ke {file_name_base}.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# URL dari halaman utama CNNIndonesia.com\n",
        "url = 'https://www.cnnindonesia.com/'\n",
        "\n",
        "print(\"Mengakses URL:\", url)\n",
        "# Mengirim permintaan HTTP ke URL target\n",
        "try:\n",
        "    # Menambahkan header User-Agent untuk menyamar sebagai browser biasa\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error saat mengakses URL: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Parsing HTML dari halaman web menggunakan BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Menyiapkan list untuk menampung semua data\n",
        "collected_data = []\n",
        "\n",
        "# Menemukan semua artikel. Di CNN Indonesia, artikel juga dibungkus dalam tag <article>\n",
        "articles = soup.find_all('article')\n",
        "print(f\"Menemukan {len(articles)} total artikel.\")\n",
        "\n",
        "for article in articles:\n",
        "    # Mencari judul (h2)\n",
        "    title_tag = article.find('h2')\n",
        "    if not title_tag:\n",
        "        continue\n",
        "\n",
        "    title = title_tag.get_text(strip=True)\n",
        "\n",
        "    # Mencari link dari tag <a> yang membungkus artikel\n",
        "    link_tag = article.find('a')\n",
        "    link = link_tag['href'] if link_tag and link_tag.has_attr('href') else 'Tidak ada link'\n",
        "\n",
        "    # 1. Mengambil URL Gambar Utama (Image URL)\n",
        "    # Gambar biasanya ada di dalam tag <img>\n",
        "    image_tag = article.find('img')\n",
        "    image_url = image_tag['src'] if image_tag and image_tag.has_attr('src') else 'Tidak ada gambar'\n",
        "\n",
        "    # 2. Mengambil Kategori Berita\n",
        "    # Kategori seringkali ada di dalam span dengan class 'text-cnn_red'\n",
        "    category_tag = article.find('span', class_='text-cnn_red')\n",
        "    category = category_tag.get_text(strip=True) if category_tag else 'Tidak ada kategori'\n",
        "\n",
        "    # Waktu, penulis, dan ringkasan tidak selalu ada di halaman utama, jadi kita set default\n",
        "    publication_time = 'Tidak ada waktu'\n",
        "    author = 'Tidak diketahui'\n",
        "    summary = 'Tidak ada ringkasan'\n",
        "\n",
        "\n",
        "    # Mengumpulkan data jika judul dan link ditemukan\n",
        "    if title and link != 'Tidak ada link':\n",
        "        article_data = {\n",
        "            'title': title,\n",
        "            'link': link,\n",
        "            'category': category,\n",
        "            'publication_time': publication_time,\n",
        "            'author': author,\n",
        "            'image_url': image_url,\n",
        "            'summary': summary\n",
        "        }\n",
        "        collected_data.append(article_data)\n",
        "\n",
        "# --- Proses Ekspor Hasil ---\n",
        "if not collected_data:\n",
        "    print(\"Tidak ada data yang berhasil di-scrape. Mungkin struktur website telah berubah.\")\n",
        "else:\n",
        "    print(f\"Berhasil mengumpulkan {len(collected_data)} artikel yang valid. Mengekspor ke file...\")\n",
        "\n",
        "    # Mengubah nama file output\n",
        "    file_name_base = 'list_post_cnn_lengkap'\n",
        "\n",
        "    output_json_data = {\"list_post\": collected_data}\n",
        "    with open(f'{file_name_base}.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_json_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Data berhasil diekspor ke {file_name_base}.json\")\n",
        "\n",
        "    df = pd.DataFrame(collected_data)\n",
        "    df.to_csv(f'{file_name_base}.csv', index=False, encoding='utf-8')\n",
        "    print(f\"Data berhasil diekspor ke {file_name_base}.csv\")\n",
        "\n",
        "    df.to_excel(f'{file_name_base}.xlsx', index=False)\n",
        "    print(f\"Data berhasil diekspor ke {file_name_base}.xlsx\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXg_U8y2sQnQ",
        "outputId": "dbbee96a-d6e3-45c2-ce97-ba9a4435de23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mengakses URL: https://www.cnnindonesia.com/\n",
            "Menemukan 104 total artikel.\n",
            "Berhasil mengumpulkan 30 artikel yang valid. Mengekspor ke file...\n",
            "Data berhasil diekspor ke list_post_cnn_lengkap.json\n",
            "Data berhasil diekspor ke list_post_cnn_lengkap.csv\n",
            "Data berhasil diekspor ke list_post_cnn_lengkap.xlsx\n"
          ]
        }
      ]
    }
  ]
}